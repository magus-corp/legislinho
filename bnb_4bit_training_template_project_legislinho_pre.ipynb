{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Installation\n",
    "(Start here if it is your 1st time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuXIFTFapAMI",
    "outputId": "37aeafbe-7856-4739-ba03-efb414fa1434",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q -U bitsandbytes\n",
    "#!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "#!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "#!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "#!pip install -q -U datasets\n",
    "#!pip install -q -U sentencepiece\n",
    "#!pip install -q -U PyPDF2\n",
    "#!pip install -q -U wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports \n",
    "(Start here if its not your 1st time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import transformers\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"legislinho\"\n",
    "save_base_path = \"trainings\"\n",
    "ts = str(datetime.timestamp(datetime.now())).replace(\".\",\"\")\n",
    "model_id_tokenizer = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "model_id = \"./legislinhOpenHermesFinalPreTraining\"\n",
    "model_id_tokenizer = \"lrds-code/samba-1.1B\"\n",
    "model_id = \"./legislinhoSambaFinalPreTraining\"\n",
    "#max_lenght=0\n",
    "#model_id = \"lrds-code/samba-1.1B\"\n",
    "#max_lenght=2048\n",
    "# BnB Config\n",
    "load_in_4bit=True\n",
    "bnb_4bit_use_double_quant=True\n",
    "bnb_4bit_quant_type=\"nf4\"\n",
    "bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# LoraConfig\n",
    "r=64\n",
    "lora_alpha=128\n",
    "target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
    "lora_dropout=0.05\n",
    "bias=\"none\"\n",
    "task_type=\"CAUSAL_LM\"\n",
    "# Dataset\n",
    "type=\"json\"\n",
    "data_files = \"./datasets_jsons_pretraining_2000/*\"\n",
    "# Encoder function\n",
    "truncation=True\n",
    "padding='max_length'\n",
    "# Trainer Args\n",
    "per_device_train_batch_size=8\n",
    "gradient_accumulation_steps=32\n",
    "warmup_steps=2\n",
    "num_train_epochs=9\n",
    "learning_rate=2e-4\n",
    "fp16=True\n",
    "logging_steps=1\n",
    "optim=\"paged_adamw_8bit\"\n",
    "save_strategy=\"epoch\"\n",
    "load_best_model_at_end=True\n",
    "# Save after training\n",
    "save_adapter=True\n",
    "save_config=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=project,\n",
    "    config={\n",
    "        \"save_base_path\": save_base_path,\n",
    "        \"ts\": ts,\n",
    "        \"model_id\": model_id,\n",
    "        \"load_in_4bit\": load_in_4bit,\n",
    "        \"bnb_4bit_use_double_quant\": bnb_4bit_use_double_quant,\n",
    "        \"bnb_4bit_quant_type\": bnb_4bit_quant_type,\n",
    "        \"bnb_4bit_compute_dtype\": bnb_4bit_compute_dtype,\n",
    "        \"lora_rank\": r,\n",
    "        \"lora_alpha\": lora_alpha,\n",
    "        \"target_modules\": target_modules,\n",
    "        \"lora_dropout\": lora_dropout,\n",
    "        \"bias\": bias,\n",
    "        \"task_type\": task_type,\n",
    "        \"type\": type,\n",
    "        \"data_files\": data_files,\n",
    "        \"truncation\": truncation,\n",
    "        \"padding\": padding,\n",
    "        \"per_device_train_batch_size\": per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"fp16\": fp16,\n",
    "        \"logging_steps\": logging_steps,\n",
    "        \"optim\": optim,\n",
    "        \"save_strategy\": save_strategy,\n",
    "        \"load_best_model_at_end\": load_best_model_at_end,\n",
    "        \"save_adapter\": save_adapter,\n",
    "        \"save_config\": save_config\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "2f968fee-906c-423e-9fd1-169f080a2ef0"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    vanilla huggingface implementation\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybeyl20n3dYH"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=target_modules, # adiciona nos modulos locos da vida\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=bias,\n",
    "    task_type=task_type\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(examples[\"data\"],  truncation=truncation, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6f4z8EYmcJ6"
   },
   "outputs": [],
   "source": [
    "data = load_dataset(type,data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda samples: encode(samples), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = save_base_path+\"/\"+project+\"-\"+ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.checkpoint.use_reentrant=False # check if this working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "KoWxN1IhXrEc",
    "outputId": "8fde6e92-4c01-4d03-ab12-1101247e74d9"
   },
   "outputs": [],
   "source": [
    "trainer_args = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=fp16,\n",
    "        logging_steps=logging_steps,\n",
    "        output_dir=output_dir,\n",
    "        optim=optim,\n",
    "        save_strategy=save_strategy,\n",
    "#        load_best_model_at_end=load_best_model_at_end,\n",
    "        report_to=\"wandb\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq0nX33BmfaC"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args = trainer_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), # find a way to generalize\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiB0xAr_Ymji"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir+\"-final\", save_adapter=save_adapter, save_config=save_config)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
